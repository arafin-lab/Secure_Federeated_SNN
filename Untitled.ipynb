{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import os\n",
    "from SNN import SNNetwork\n",
    "from utils.training_utils import train, get_acc_and_loss\n",
    "import time\n",
    "import numpy as np\n",
    "import tables\n",
    "import argparse\n",
    "import utils\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "import utils.filters as filters\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_path = '/home/cream/Desktop/arafin_experiments/SOCC/FL-SNN/data/'\n",
    "save_path = os.getcwd() + r'/results'\n",
    "\n",
    "datasets = {\n",
    "            'mnist_dvs_10': r'mnist_dvs_25ms_26pxl_10_digits.hdf5'\n",
    "            }\n",
    "\n",
    "\n",
    "dataset = local_data_path + datasets['mnist_dvs_10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the training dataset: torch.Size([9000, 676, 80])\n",
      "Shape of the test dataset torch.Size([1000, 676, 80])\n"
     ]
    }
   ],
   "source": [
    "input_train = torch.FloatTensor(tables.open_file(dataset).root.train.data[:])\n",
    "output_train = torch.FloatTensor(tables.open_file(dataset).root.train.label[:])\n",
    "\n",
    "input_test = torch.FloatTensor(tables.open_file(dataset).root.test.data[:])\n",
    "output_test = torch.FloatTensor(tables.open_file(dataset).root.test.label[:])\n",
    "\n",
    "### sanity check\n",
    "print(\"Shape of the training dataset:\", input_train.shape)\n",
    "print(\"Shape of the test dataset\", input_test.shape)\n",
    "\n",
    "### Network parameters\n",
    "n_input_neurons = input_train.shape[1]\n",
    "n_output_neurons = output_train.shape[1]\n",
    "n_hidden_neurons = 4\n",
    "epochs = input_train.shape[0]\n",
    "epochs_test = input_test.shape[0]\n",
    "\n",
    "test_accs = []\n",
    "\n",
    "learning_rate = 0.005 / n_hidden_neurons\n",
    "kappa = 0.2\n",
    "alpha = 1\n",
    "deltas = 1\n",
    "num_ite = 1\n",
    "r = 0.3\n",
    "weights_magnitude=0.05\n",
    "task='supervised'\n",
    "mode='train', \n",
    "tau_ff=10\n",
    "tau_fb=10\n",
    "mu=1.5, \n",
    "n_basis_feedforward = 8\n",
    "feedforward_filter = filters.raised_cosine_pillow_08\n",
    "feedback_filter = filters.raised_cosine_pillow_08\n",
    "n_basis_feedback = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 676, 80]) torch.Size([100, 676, 80]) torch.Size([100, 676, 80])\n",
      "torch.Size([10, 676, 80]) torch.Size([10, 676, 80]) torch.Size([10, 676, 80])\n"
     ]
    }
   ],
   "source": [
    "#divide the training and test data into 10 segments\n",
    "\n",
    "user1_train_data = input_train[0:100,:,:]\n",
    "user2_train_data =  input_train[100:200,:,:]\n",
    "user3_train_data = input_train[200:300,:,:]\n",
    "print(user1_train_data.shape,user2_train_data.shape, user3_train_data.shape)\n",
    "user1_test_data = input_test[0:10,:,:]\n",
    "user2_test_data =  input_test[10:20,:,:]\n",
    "user3_test_data = input_test[20:30,:,:]\n",
    "print(user1_test_data.shape,user2_test_data.shape, user3_test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "topology = torch.ones([n_hidden_neurons + n_output_neurons, n_input_neurons + n_hidden_neurons + n_output_neurons], dtype=torch.float)\n",
    "topology[[i for i in range(n_output_neurons + n_hidden_neurons)], [i + n_input_neurons for i in range(n_output_neurons + n_hidden_neurons)]] = 0\n",
    "assert torch.sum(topology[:, :n_input_neurons]) == (n_input_neurons * (n_hidden_neurons + n_output_neurons))\n",
    "print(topology[:, n_input_neurons:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start distributed training\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import os\n",
    "from utils.training_utils import local_feedback_and_update, feedforward_sampling, get_acc_and_loss\n",
    "from utils.distributed_utils import init_training, global_update, init_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    filters_dict = {'base_ff_filter': filters.base_feedforward_filter, 'base_fb_filter': filters.base_feedback_filter, 'cosine_basis': filters.cosine_basis,\n",
    "                    'raised_cosine': filters.raised_cosine, 'raised_cosine_pillow_05': filters.raised_cosine_pillow_05, 'raised_cosine_pillow_08': filters.raised_cosine_pillow_08}\n",
    "\n",
    "    network_parameters = {'n_input_neurons': n_input_neurons,\n",
    "                          'n_hidden_neurons': n_hidden_neurons,\n",
    "                          'n_output_neurons': n_output_neurons,\n",
    "                          'topology': topology,\n",
    "                          'n_basis_feedforward': n_basis_feedforward,\n",
    "                          'feedforward_filter': feedforward_filter,\n",
    "                          'n_basis_feedback': 1,\n",
    "                          'feedback_filter': feedforward_filter,\n",
    "                          'tau_ff': tau_ff,\n",
    "                          'tau_fb': tau_ff,\n",
    "                          'mu': mu,\n",
    "                          'weights_magnitude': weights_magnitude,\n",
    "                          'save_path': save_path\n",
    "                          }\n",
    "\n",
    "    training_parameters = {'dataset': dataset,\n",
    "                           'tau': 10,\n",
    "                           'learning_rate': learning_rate,\n",
    "                           'epochs': epochs,\n",
    "                           'epochs_test': epochs_test,\n",
    "                           'eta': 1,\n",
    "                           'kappa': kappa,\n",
    "                           'deltas': deltas,\n",
    "                           'alpha':alpha,\n",
    "                           'r': r,\n",
    "                           'num_ite': num_ite\n",
    "                           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training at node 0\n",
      "At network\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.]])\n",
      "self.n_neurons are 690\n",
      "training at node 1\n",
      "Hi [[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]At network\n",
      "\n",
      "tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.]])training at node 2\n",
      "\n",
      "training at node 3\n",
      "self.n_neurons are 690\n",
      "At network\n"
     ]
    }
   ],
   "source": [
    "from torch.multiprocessing import Process\n",
    "import datetime\n",
    "\n",
    "\n",
    "def init_training(rank, num_nodes, nodes_group, dataset, eta, epochs, net_parameters):\n",
    "    print(\"At init_training\")\n",
    "    \"\"\"\"\n",
    "    Initializes the different parameters for distributed training\n",
    "    \"\"\"\n",
    "    # Initialize an SNN\n",
    "    network = SNNetwork(**net_parameters)\n",
    "\n",
    "    # At the beginning, the master node:\n",
    "    # - transmits its weights to the workers\n",
    "    # - distributes the samples among workers\n",
    "    if rank == 0:\n",
    "        # Initializing an aggregation list for future weights collection\n",
    "        weights_list = [[torch.zeros(network.feedforward_weights.shape, dtype=torch.float) for _ in range(num_nodes)],\n",
    "                        [torch.zeros(network.feedback_weights.shape, dtype=torch.float) for _ in range(num_nodes)],\n",
    "                        [torch.zeros(network.bias.shape, dtype=torch.float) for _ in range(num_nodes)],\n",
    "                        [torch.zeros(1, dtype=torch.float) for _ in range(num_nodes)]]\n",
    "    else:\n",
    "        weights_list = []\n",
    "    print(weights_list)\n",
    "    dist.barrier(nodes_group)\n",
    "    # Randomly create a distribution of the training samples among nodes\n",
    "    local_training_sequence = distribute_samples(nodes_group, rank, dataset, eta, epochs)\n",
    "    S_prime = local_training_sequence.shape[-1]\n",
    "    S = S_prime * epochs\n",
    "\n",
    "    dist.barrier(nodes_group)\n",
    "\n",
    "    # Master node sends its weights\n",
    "    for parameter in network.get_parameters():\n",
    "        dist.broadcast(network.get_parameters()[parameter], 0, group=nodes_group)\n",
    "    if rank == 0:\n",
    "        print('Node 0 has shared its model and training data is partitioned among workers')\n",
    "\n",
    "    # The nodes initialize their eligibility trace and learning signal\n",
    "    eligibility_trace = {'ff_weights': 0, 'fb_weights': 0, 'bias': 0}\n",
    "    et_temp = {'ff_weights': 0, 'fb_weights': 0, 'bias': 0}\n",
    "\n",
    "    learning_signal = 0\n",
    "    ls_temp = 0\n",
    "\n",
    "    return network, local_training_sequence, weights_list, S_prime, S, eligibility_trace, et_temp, learning_signal, ls_temp\n",
    "\n",
    "\n",
    "def distribute_samples(nodes, rank, dataset, eta, epochs):\n",
    "    \"\"\"\n",
    "    The master node (rank 0) randomly chooses and transmits samples indices to each device for training.\n",
    "    Upon reception of their assigned samples, the nodes create their training dataset\n",
    "    \"\"\"\n",
    "\n",
    "    if rank == 0:\n",
    "        inpi = tables.open_file(dataset).root.train.data.shape[0]\n",
    "\n",
    "        print(inpi)\n",
    "        n_samples = tables.open_file(dataset).root.train.data.shape[0]  # Total number of samples\n",
    "        n_samples_train_per_class = int(n_samples / 2 * 0.9)  # There are 2 classes and 10% of the dataset is kept for testing\n",
    "\n",
    "        # Indices corresponding to each class\n",
    "        indices_0 = np.asarray(torch.max(torch.sum(torch.FloatTensor(tables.open_file(dataset).root.train.label[:]), dim=-1), dim=-1).indices == 0).nonzero()[0][:n_samples_train_per_class]\n",
    "        indices_1 = np.asarray(torch.max(torch.sum(torch.FloatTensor(tables.open_file(dataset).root.train.label[:]), dim=-1), dim=-1).indices == 1).nonzero()[0][:n_samples_train_per_class]\n",
    "\n",
    "        assert len(indices_0) == len(indices_1)\n",
    "        n_main_class = math.floor(epochs * eta)\n",
    "        n_secondary_class = epochs - n_main_class\n",
    "        assert (n_main_class + n_secondary_class) == epochs\n",
    "\n",
    "        # Randomly select samples for each worker\n",
    "        indices_worker_0 = np.hstack((np.random.choice(indices_0, [n_main_class], replace=False), np.random.choice(indices_1, [n_secondary_class], replace=False)))\n",
    "        np.random.shuffle(indices_worker_0)\n",
    "        remaining_indices_0 = [i for i in indices_0 if i not in indices_worker_0]\n",
    "        remaining_indices_1 = [i for i in indices_1 if i not in indices_worker_0]\n",
    "        indices_worker_1 = np.hstack((np.random.choice(remaining_indices_0, [n_secondary_class], replace=False), np.random.choice(remaining_indices_1, [n_main_class], replace=False)))\n",
    "        np.random.shuffle(indices_worker_1)\n",
    "\n",
    "        assert len(indices_worker_0) == len(indices_worker_1)\n",
    "\n",
    "        # Send samples to the workers\n",
    "        indices = [torch.zeros([epochs], dtype=torch.int), torch.IntTensor(indices_worker_0), torch.IntTensor(indices_worker_1)]\n",
    "        indices_local = torch.zeros([epochs], dtype=torch.int)\n",
    "        dist.scatter(tensor=indices_local, src=0, scatter_list=indices, group=nodes)\n",
    "\n",
    "        # Save samples sent to the workers at master to evaluate train loss and accuracy later\n",
    "        indices_local = torch.IntTensor(np.hstack((indices_worker_0, indices_worker_1)))\n",
    "        local_input = tables.open_file(dataset).root.train.data[:][indices_local]\n",
    "        local_output = tables.open_file(dataset).root.train.label[:][indices_local]\n",
    "        local_teaching_signal = torch.cat((torch.FloatTensor(local_input), torch.FloatTensor(local_output)), dim=1)\n",
    "\n",
    "    else:\n",
    "        indices_local = torch.zeros([epochs], dtype=torch.int)\n",
    "        dist.scatter(tensor=indices_local, src=0, scatter_list=[], group=nodes)\n",
    "\n",
    "        assert torch.sum(indices_local) != 0\n",
    "\n",
    "        local_input = tables.open_file(dataset).root.train.data[:][indices_local]\n",
    "        local_output = tables.open_file(dataset).root.train.label[:][indices_local]\n",
    "\n",
    "        local_teaching_signal = torch.cat((torch.FloatTensor(local_input), torch.FloatTensor(local_output)), dim=1)\n",
    "\n",
    "    return local_teaching_signal\n",
    "\n",
    "\n",
    "def feedforward_sampling_accum_gradients(network, training_sequence, et, ls, gradients_accum, s, S_prime, alpha, r):\n",
    "    \"\"\"\"\n",
    "    Runs a feedforward sampling pass:\n",
    "    - computes log probabilities\n",
    "    - accumulates learning signal\n",
    "    - accumulates eligibility trace,\n",
    "    and accumulates gradients during the procedure\n",
    "    \"\"\"\n",
    "    # Run forward pass\n",
    "    log_proba = network(training_sequence[int(s / S_prime), :, s % S_prime])\n",
    "\n",
    "    # Accumulate learning signal\n",
    "    ls += torch.sum(log_proba[network.output_neurons - network.n_non_learnable_neurons]) / network.n_learnable_neurons \\\n",
    "          - alpha*torch.sum(network.spiking_history[network.hidden_neurons, -1]\n",
    "          * torch.log(1e-07 + torch.sigmoid(network.potential[network.hidden_neurons - network.n_non_learnable_neurons]) / r)\n",
    "          + (1 - network.spiking_history[network.hidden_neurons, -1])\n",
    "          * torch.log(1e-07 + (1. - torch.sigmoid(network.potential[network.hidden_neurons - network.n_non_learnable_neurons])) / (1 - r))) / network.n_learnable_neurons\n",
    "\n",
    "    # Accumulate eligibility trace\n",
    "    for parameter in et:\n",
    "        if parameter == 'ff_weights':\n",
    "            gradients_accum += torch.abs(network.get_gradients()[parameter])\n",
    "        et[parameter] += network.get_gradients()[parameter]\n",
    "\n",
    "    return log_proba, gradients_accum, ls, et\n",
    "\n",
    "\n",
    "def global_update(nodes, rank, network, weights_list):\n",
    "    \"\"\"\"\n",
    "    Global update step for distributed learning.\n",
    "    \"\"\"\n",
    "\n",
    "    for j, parameter in enumerate(network.get_parameters()):\n",
    "        if rank != 0:\n",
    "            dist.gather(tensor=network.get_parameters()[parameter].data, gather_list=[], dst=0, group=nodes)\n",
    "        else:\n",
    "            dist.gather(tensor=network.get_parameters()[parameter].data, gather_list=weights_list[j], dst=0, group=nodes)\n",
    "            network.get_parameters()[parameter].data = torch.mean(torch.stack(weights_list[j][1:]), dim=0)\n",
    "        dist.broadcast(network.get_parameters()[parameter], 0, group=nodes)\n",
    "\n",
    "\n",
    "def global_update_subset(nodes, rank, network, weights_list, gradients_accum, n_weights_to_send):\n",
    "    \"\"\"\"\n",
    "    Global update step for distributed learning when transmitting only a subset of the weights.\n",
    "    Each worker node transmits a tensor in which only the indices corresponding to the synapses with the largest n_weights_to_send accumulated gradients are kept nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    for j, parameter in enumerate(network.get_parameters()):\n",
    "        if j == 0:\n",
    "            if rank != 0:\n",
    "                to_send = network.get_parameters()[parameter].data  # each worker node copies its weights in a new vector\n",
    "                # Selection of the indices to set to zero before transmission\n",
    "                indices_not_to_send = [i for i in range(network.n_basis_feedforward) if i not in torch.topk(torch.sum(gradients_accum, dim=(0, 1)), n_weights_to_send)[1]]\n",
    "                to_send[:, :, indices_not_to_send] = 0\n",
    "\n",
    "                # Transmission of the quantized weights\n",
    "                dist.gather(tensor=to_send, gather_list=[], dst=0, group=nodes)\n",
    "            else:\n",
    "                dist.gather(tensor=network.get_parameters()[parameter].data, gather_list=weights_list[j], dst=0, group=nodes)\n",
    "\n",
    "                indices_received = torch.bincount(torch.nonzero(torch.sum(torch.stack(weights_list[j][1:]), dim=(1, 2)))[:, 1])\n",
    "                multiples = torch.zeros(network.n_basis_feedforward)  # indices of weights transmitted by two devices at once: those will be averaged\n",
    "                multiples[:len(indices_received)] = indices_received\n",
    "                multiples[multiples == 0] = 1\n",
    "\n",
    "                # Averaging step\n",
    "                network.get_parameters()[parameter].data = torch.sum(torch.stack(weights_list[j][1:]), dim=0) / multiples.type(torch.float)\n",
    "\n",
    "        else:\n",
    "            if rank != 0:\n",
    "                dist.gather(tensor=network.get_parameters()[parameter].data, gather_list=[], dst=0, group=nodes)\n",
    "            else:\n",
    "                dist.gather(tensor=network.get_parameters()[parameter].data, gather_list=weights_list[j], dst=0, group=nodes)\n",
    "                network.get_parameters()[parameter].data = torch.mean(torch.stack(weights_list[j][1:]), dim=0)\n",
    "        dist.broadcast(network.get_parameters()[parameter], 0, group=nodes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run(rank,size,net_params,train_params):\n",
    "    #print('at rank',rank, 'training params', training_parameters,'network_parameters',network_parameters)\n",
    "    # Setup training parameters\n",
    "    dataset = train_params['dataset']\n",
    "    epochs = train_params['epochs']\n",
    "    epochs_test = train_params['epochs_test']\n",
    "    deltas = train_params['deltas']\n",
    "    num_ite = train_params['num_ite']\n",
    "    save_path = net_params['save_path']\n",
    "    tau = train_params['tau']\n",
    "\n",
    "    learning_rate = train_params['learning_rate']\n",
    "    alpha = train_params['alpha']\n",
    "    eta = train_params['eta']\n",
    "    kappa = train_params['kappa']\n",
    "    r = train_params['r']\n",
    "\n",
    "    # Create network groups for communication\n",
    "    all_nodes = dist.new_group([0, 1, 2,3], timeout=datetime.timedelta(0, 360000))\n",
    "\n",
    "    test_accuracies = []  # used to store test accuracies\n",
    "    test_loss = [[] for _ in range(num_ite)]\n",
    "    test_indices = np.hstack((np.arange(900, 1000)[:epochs_test], np.arange(1900, 2000)[:epochs_test]))\n",
    "\n",
    "    print('training at node', rank)\n",
    "    network = SNNetwork(**net_params)\n",
    "    dist.barrier(all_nodes)\n",
    "\n",
    "    \n",
    "def init_process(rank, size, network_parameters,training_parameters,fn, backend='gloo'):\n",
    "    os.environ['MASTER_ADDR']='127.0.0.1'\n",
    "    os.environ['MASTER_PORT']='29501'\n",
    "    dist.init_process_group(backend,rank=rank, world_size=size)\n",
    "    fn(rank,size,network_parameters,training_parameters)\n",
    "size=4\n",
    "processes =[]\n",
    "for rank in range(size):\n",
    "    p= Process(target=init_process, args=(rank, size,network_parameters, training_parameters, run))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for p in processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
